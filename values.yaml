global:
  image: minh5/airflow
  imageTag: 0.1.0
  pullPolicy: Always
  restartPolicy: Always

rbac:
  namespace: default
  role:
    kind: ClusterRole
    name: cluster-admin
    apiGroup: rbac.authorization.k8s.io

app:
  name: webserver
  replicas: 1
  servicePort: 8080
  containerPort: 8080
  urlPath: /airflow

persistence:
  enabled: true
  logs:
    name: airflow-logs
    accessMode: ReadWriteMany
    size: 2Gi
    path: /root/airflow/logs
  dags:
    name: airflow-dags
    accessMode: ReadWriteOnce
    size: 2Gi
    path: /root/airflow/dags
  
probe:
  enabled: False
  readiness:
    delaySeconds: 5 
    timeoutSeconds: 5
    periodSeconds: 5
    http: 
      path: /login
      port: 8080
  liveness:
    delaySeconds: 5
    timeoutSeconds: 5
    failureThreshold: 5
    http:
      path: /login
      port: 8080

service:
  type: NodePort
  port: 8080
  nodePort: 30809

initFile: |-
  airflow initdb
  bash /tmp/config/config.sh
  bash /usr/local/bin/airflow-init $@

configFile: |-
  [core]
  airflow_home = /root/airflow
  dags_folder = /root/airflow/dags
  base_log_folder = /root/airflow/logs
  logging_level = INFO
  executor = KubernetesExecutor
  parallelism = 32
  load_examples = False
  plugins_folder = /root/airflow/plugins
  sql_alchemy_conn = $SQL_ALCHEMY_CONN
  [scheduler]
  dag_dir_list_interval = 300
  child_process_log_directory = /root/airflow/logs/scheduler
  # Task instances listen for external kill signal (when you clear tasks
  # from the CLI or the UI), this defines the frequency at which they should
  # listen (in seconds).
  job_heartbeat_sec = 5
  max_threads = 2
  # The scheduler constantly tries to trigger new tasks (look at the
  # scheduler section in the docs for more information). This defines
  # how often the scheduler should run (in seconds).
  scheduler_heartbeat_sec = 5
  # after how much time should the scheduler terminate in seconds
  # -1 indicates to run continuously (see also num_runs)
  run_duration = -1
  # after how much time a new DAGs should be picked up from the filesystem
  min_file_process_interval = 0
  statsd_on = False
  statsd_host = localhost
  statsd_port = 8125
  statsd_prefix = airflow
  print_stats_interval = 30
  scheduler_zombie_task_threshold = 300
  max_tis_per_query = 0
  authenticate = False
  # Turn off scheduler catchup by setting this to False.
  # Default behavior is unchanged and
  # Command Line Backfills still work, but the scheduler
  # will not do scheduler catchup if this is False,
  # however it can be set on a per DAG basis in the
  # DAG definition (catchup)
  catchup_by_default = True
  [webserver]
  # The base url of your website as airflow cannot guess what domain or
  # cname you are using. This is used in automated emails that
  # airflow sends to point links to the right web server
  base_url = http://localhost:8080
  # The ip specified when starting the web server
  web_server_host = 0.0.0.0
  # The port on which to run the web server
  web_server_port = 8080
  # Paths to the SSL certificate and key for the web server. When both are
  # provided SSL will be enabled. This does not change the web server port.
  web_server_ssl_cert =
  web_server_ssl_key =
  # Number of seconds the webserver waits before killing gunicorn master that doesn't respond
  web_server_master_timeout = 120
  # Number of seconds the gunicorn webserver waits before timing out on a worker
  web_server_worker_timeout = 120
  # Number of workers to refresh at a time. When set to 0, worker refresh is
  # disabled. When nonzero, airflow periodically refreshes webserver workers by
  # bringing up new ones and killing old ones.
  worker_refresh_batch_size = 1
  # Number of seconds to wait before refreshing a batch of workers.
  worker_refresh_interval = 30
  # Secret key used to run your flask app
  secret_key = temporary_key
  # Number of workers to run the Gunicorn web server
  workers = 4
  # The worker class gunicorn should use. Choices include
  # sync (default), eventlet, gevent
  worker_class = sync
  # Log files for the gunicorn webserver. '-' means log to stderr.
  access_logfile = -
  error_logfile = -
  # Expose the configuration file in the web server
  expose_config = False
  # Set to true to turn on authentication:
  # https://airflow.incubator.apache.org/security.html#web-authentication
  authenticate = False
  # Filter the list of dags by owner name (requires authentication to be enabled)
  filter_by_owner = False
  # Filtering mode. Choices include user (default) and ldapgroup.
  # Ldap group filtering requires using the ldap backend
  #
  # Note that the ldap server needs the "memberOf" overlay to be set up
  # in order to user the ldapgroup mode.
  owner_mode = user
  # Default DAG view.  Valid values are:
  # tree, graph, duration, gantt, landing_times
  dag_default_view = tree
  # Default DAG orientation. Valid values are:
  # LR (Left->Right), TB (Top->Bottom), RL (Right->Left), BT (Bottom->Top)
  dag_orientation = LR
  # Puts the webserver in demonstration mode; blurs the names of Operators for
  # privacy.
  demo_mode = False
  # The amount of time (in secs) webserver will wait for initial handshake
  # while fetching logs from other worker machine
  log_fetch_timeout_sec = 5
  # By default, the webserver shows paused DAGs. Flip this to hide paused
  # DAGs by default
  hide_paused_dags_by_default = False
  # Consistent page size across all listing views in the UI
  page_size = 100
  # Use FAB-based webserver with RBAC feature
  rbac = True
  [smtp]
  # If you want airflow to send emails on retries, failure, and you want to use
  # the airflow.utils.email.send_email_smtp function, you have to configure an
  # smtp server here
  smtp_host = localhost
  smtp_starttls = True
  smtp_ssl = False
  # Uncomment and set the user/pass settings if you want to use SMTP AUTH
  # smtp_user = airflow
  # smtp_password = airflow
  smtp_port = 25
  smtp_mail_from = airflow@example.com
  [kubernetes]
  airflow_configmap = airflow-configmap
  worker_container_repository = airflow
  worker_container_tag = latest
  worker_container_image_pull_policy = IfNotPresent
  worker_dags_folder = /tmp/dags
  delete_worker_pods = True
  git_repo = https://github.com/apache/incubator-airflow.git
  git_branch = master
  git_subpath = airflow/example_dags/
  git_user =
  git_password =
  dags_volume_claim = airflow-dags
  dags_volume_subpath =
  logs_volume_claim = airflow-logs
  logs_volume_subpath =
  in_cluster = True
  namespace = default
  gcp_service_account_keys =
  # For cloning DAGs from git repositories into volumes: https://github.com/kubernetes/git-sync
  git_sync_container_repository = gcr.io/google-containers/git-sync-amd64
  git_sync_container_tag = v2.0.5
  git_sync_init_container_name = git-sync-clone
  [kubernetes_node_selectors]
  # The Key-value pairs to be given to worker pods.
  # The worker pods will be scheduled to the nodes of the specified key-value pairs.
  # Should be supplied in the format: key = value
  [kubernetes_secrets]
  SQL_ALCHEMY_CONN = airflow-secrets=sql_alchemy_conn
  [cli]
  api_client = airflow.api.client.json_client
  endpoint_url = http://localhost:8080
  [api]
  auth_backend = airflow.api.auth.backend.default
  [github_enterprise]
  api_rev = v3
  [admin]
  # UI to hide sensitive variable fields when set to True
  hide_sensitive_variable_fields = True
